{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ebff93b22ede4bfa91250845749e6b5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_68e645ef4c9f47729d0d2792ae386312",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4fc9928881be49d5ba4e0f0a1af00ab0",
              "IPY_MODEL_26760fab1f95431a903e716890b12940"
            ]
          }
        },
        "68e645ef4c9f47729d0d2792ae386312": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4fc9928881be49d5ba4e0f0a1af00ab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_60a38b0ee2d045b7a7553d1d5abc5045",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 973,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 973,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8c97b44c2cb842a681ab00033f971232"
          }
        },
        "26760fab1f95431a903e716890b12940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4b8e6477899843bb96ad273a3b096c11",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 973/973 [04:07&lt;00:00,  3.93it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c141a282594947f7864bea8dae198e8e"
          }
        },
        "60a38b0ee2d045b7a7553d1d5abc5045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8c97b44c2cb842a681ab00033f971232": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b8e6477899843bb96ad273a3b096c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c141a282594947f7864bea8dae198e8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "439e8afb35764e1ab86d0989c8c27e7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_34297d2d8a944d75a604fbcd5b3874a7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_18152586d89d41bf9fca2ca29c84532c",
              "IPY_MODEL_a78e0e8c10d0486e92d891b5f70c6a63"
            ]
          }
        },
        "34297d2d8a944d75a604fbcd5b3874a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18152586d89d41bf9fca2ca29c84532c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_13e52acf015542ab8456b65166c479f8",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9b70bbe7f5f0481081bbba1c96a469cb"
          }
        },
        "a78e0e8c10d0486e92d891b5f70c6a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_09457583d66644a49d657b20cfa0a437",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1000/1000 [03:29&lt;00:00,  4.78it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b2650163694b4be7acb2048e51921db6"
          }
        },
        "13e52acf015542ab8456b65166c479f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9b70bbe7f5f0481081bbba1c96a469cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "09457583d66644a49d657b20cfa0a437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b2650163694b4be7acb2048e51921db6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46e24bfe7aed4a9482910f0c63cf7120": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f7665f6a5f1c40728e40fd7a330f6e4f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1250e19a8c244707a09d5a0c9c2dc538",
              "IPY_MODEL_a0769b66d0e44ada9c028d484a82405f"
            ]
          }
        },
        "f7665f6a5f1c40728e40fd7a330f6e4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1250e19a8c244707a09d5a0c9c2dc538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3ddabc34ce874b3e9b80f517c5ac42a2",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 983,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 983,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c84f71063ca416da01d8e6b8fef3fa1"
          }
        },
        "a0769b66d0e44ada9c028d484a82405f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_17b9f9615dca49ccbb0e9482987f5891",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 983/983 [01:25&lt;00:00, 11.45it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b9f3734d4b42496d8e1eb32e86080869"
          }
        },
        "3ddabc34ce874b3e9b80f517c5ac42a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c84f71063ca416da01d8e6b8fef3fa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "17b9f9615dca49ccbb0e9482987f5891": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b9f3734d4b42496d8e1eb32e86080869": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c7e2d5cb7b6431ea160fc9b59a762e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cb4109b722c24db686e233cb011127ce",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8d1399f3f4e349e98387d0d22f2b61b4",
              "IPY_MODEL_15ee459b05a54f088173e81337322c17"
            ]
          }
        },
        "cb4109b722c24db686e233cb011127ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d1399f3f4e349e98387d0d22f2b61b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a54fc994f45f40e89fbe9b233dac711e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 973,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 973,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5768b8aa8e304a3189c82511696a8ecc"
          }
        },
        "15ee459b05a54f088173e81337322c17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bcf086333e784173b5bb0666601128e3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 973/973 [05:02&lt;00:00,  3.21it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_33fa8035453d4c7fb338ed82020b360e"
          }
        },
        "a54fc994f45f40e89fbe9b233dac711e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5768b8aa8e304a3189c82511696a8ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bcf086333e784173b5bb0666601128e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "33fa8035453d4c7fb338ed82020b360e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "krgXGcnD-0F5"
      },
      "source": [
        "import re\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwcmqMCGDinS"
      },
      "source": [
        "!cp /content/drive/MyDrive/NLP/*.pickle .\n",
        "!cp /content/drive/MyDrive/NLP/*.pth ."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzeloVZb-2GC"
      },
      "source": [
        "## Generated text\n",
        "pickle_gut = open(\"generated_text_guten.pickle\",\"rb\")\n",
        "gen_text_gut = pickle.load(pickle_gut)\n",
        "\n",
        "pickle_brow = open(\"generated_text_brown.pickle\",\"rb\")\n",
        "gen_text_brown = pickle.load(pickle_brow)\n",
        "\n",
        "## Real world Text\n",
        "## Using Movie plots from IMDB\n",
        "\n",
        "# read pickle file\n",
        "pickle_in = open(\"plots_text.pickle\",\"rb\")\n",
        "movie_plots = pickle.load(pickle_in)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHG1tXTCwaof"
      },
      "source": [
        "class data_preprocessor():\n",
        "  def __init__(self):\n",
        "    self.corpus = []\n",
        "    self.sequences = []\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.input_text = []\n",
        "    self.output_text = []\n",
        "    self.input_idx = []\n",
        "    self.output_idx = []\n",
        "\n",
        "  def clean_corpus(self, corpus):\n",
        "    if self.ss:\n",
        "      print(\"Cleaning the corpus..\")\n",
        "    self.corpus = [re.sub(\"[^a-z' ]\", \"\", i) for i in corpus]\n",
        "\n",
        "  def create_sequence(self, seq_len = 5, hide_progress=True):\n",
        "    \"\"\"\n",
        "    Function to create sequences of a given length from a corpus\n",
        "    \"\"\"\n",
        "    if self.ss:\n",
        "      print(f\"\\nGenerating sequences of length {seq_len} from the corpus..\")\n",
        "    for line in tqdm(self.corpus, desc=\"Process lines \", disable=hide_progress):\n",
        "      token_list = line.split()\n",
        "      token_list = [token for token in token_list if token != \"\" or token != \" \"]\n",
        "      token_list_len = len(token_list)\n",
        "      if token_list_len > seq_len:\n",
        "        for i in range(seq_len,token_list_len):\n",
        "          seq = token_list[i-seq_len:i+1]\n",
        "          self.sequences.append(\" \".join(seq))\n",
        "      # else:\n",
        "      #   self.sequences.append(line)\n",
        "\n",
        "    if self.ss:\n",
        "      print(f\"Generated {len(self.sequences)} sequences !\")\n",
        "\n",
        "  def create_train_data(self):\n",
        "    \"\"\"\n",
        "    Function to generate input and output text\n",
        "    Input text : all words in the sentence except the last one\n",
        "    Output text : all words in the sentence except the first one\n",
        "    \"\"\"\n",
        "    if self.ss:\n",
        "      print(\"\\nGenerating Input and Ouput sequences..\")\n",
        "    for seq in self.sequences:\n",
        "      self.input_text.append(\" \".join(seq.split()[:-1]))\n",
        "      self.output_text.append(\" \".join(seq.split()[1:]))\n",
        "\n",
        "\n",
        "  def generate_wordindex_map(self):\n",
        "    \"\"\"\n",
        "    Function to generate index to word and word to index mapping\n",
        "    \"\"\"\n",
        "    if self.ss:\n",
        "      print(\"\\nGenerating word to index and index to word mapping..\")\n",
        "    self.all_words = sorted(set(\" \".join(self.corpus).split())) \n",
        "    self.vocab_size = len(self.all_words)  \n",
        "    self.word2idx = {w: self.all_words.index(w) for w in self.all_words}\n",
        "    self.idx2word = {self.all_words.index(w): w  for w in self.all_words}\n",
        "    if self.ss:\n",
        "      print(f\"Vocab Size : {self.vocab_size}\")\n",
        "\n",
        "\n",
        "  def generate_idx_train_data(self):\n",
        "    \"\"\"\n",
        "    Function to generate indexs of corresponding input and output text\n",
        "    \"\"\"\n",
        "    if self.ss:\n",
        "      print(\"\\nGenerating Indexed version of input/output data..\")\n",
        "\n",
        "    self.input_idx = np.array([[self.word2idx[w] for w in \n",
        "                       inp_sentec.split()] for inp_sentec in self.input_text])\n",
        "    \n",
        "    self.output_idx = np.array([[self.word2idx[w] for w in \n",
        "                       out_sentec.split()] for out_sentec in self.output_text])\n",
        "\n",
        "\n",
        "\n",
        "  def process_text(self, corpus, show_status = False):\n",
        "    self.ss = show_status  \n",
        "    self.clean_corpus(corpus)\n",
        "    self.create_sequence()\n",
        "    self.create_train_data()\n",
        "    self.generate_wordindex_map()\n",
        "    self.generate_idx_train_data()\n",
        "    if self.ss:\n",
        "      print(\"\\nProcessing done !\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6ZJkjDwoJDl"
      },
      "source": [
        "# Processing Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiG6zD1ioN6I"
      },
      "source": [
        "### Real Data (IMDB Movie plots)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSJom74aelAN",
        "outputId": "48f06dd5-de41-4c58-a63a-6e72f6c897ae"
      },
      "source": [
        "data_proc = data_preprocessor()\n",
        "\n",
        "data_proc.process_text(movie_plots, show_status=True )"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaning the corpus..\n",
            "\n",
            "Generating sequences of length 5 from the corpus..\n",
            "Generated 152644 sequences !\n",
            "\n",
            "Generating Input and Ouput sequences..\n",
            "\n",
            "Generating word to index and index to word mapping..\n",
            "Vocab Size : 16592\n",
            "\n",
            "Generating Indexed version of input/output data..\n",
            "\n",
            "Processing done !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5d9-MLWn14m",
        "outputId": "9c44db82-59a3-4e87-b892-58606e2392b5"
      },
      "source": [
        "print(data_proc.input_text[1], data_proc.input_idx[1])\n",
        "print(data_proc.output_text[1], data_proc.output_idx[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is a private with the [ 7662    58 11422 16310 14867]\n",
            "a private with the st [   58 11422 16310 14867 14005]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ia41uIZoUC-"
      },
      "source": [
        "### Generated Data 1 (from NLTK Gutenberg)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK-vKNkInI7K",
        "outputId": "d359d7b0-a529-4ebb-b0eb-ed7994287d7f"
      },
      "source": [
        "data_proc_gut = data_preprocessor()\n",
        "data_proc_gut.process_text(gen_text_gut[:-1000], show_status=True )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaning the corpus..\n",
            "\n",
            "Generating sequences of length 5 from the corpus..\n",
            "Generated 90785 sequences !\n",
            "\n",
            "Generating Input and Ouput sequences..\n",
            "\n",
            "Generating word to index and index to word mapping..\n",
            "Vocab Size : 177\n",
            "\n",
            "Generating Indexed version of input/output data..\n",
            "\n",
            "Processing done !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLfD_M5Anfa5",
        "outputId": "654a9e00-8029-4380-f033-47c6a8a4223f"
      },
      "source": [
        "print(data_proc_gut.input_text[1], data_proc_gut.input_idx[1])\n",
        "print(data_proc_gut.output_text[1], data_proc_gut.output_idx[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you will be a very [175 167  24   0 157]\n",
            "will be a very good [167  24   0 157  59]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjz5ZvrYoaUU"
      },
      "source": [
        "### Generated Data 2 (from NLTK Brown)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBFgKXwgnYHe",
        "outputId": "e8221aab-73d0-44aa-e3f0-28e354e751a8"
      },
      "source": [
        "data_proc_brown = data_preprocessor()\n",
        "data_proc_brown.process_text(gen_text_brown[:-1000], show_status=True )"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaning the corpus..\n",
            "\n",
            "Generating sequences of length 5 from the corpus..\n",
            "Generated 86371 sequences !\n",
            "\n",
            "Generating Input and Ouput sequences..\n",
            "\n",
            "Generating word to index and index to word mapping..\n",
            "Vocab Size : 183\n",
            "\n",
            "Generating Indexed version of input/output data..\n",
            "\n",
            "Processing done !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Q4KDv_n7up",
        "outputId": "a35af116-b568-4871-a8c4-6e0bbba81f04"
      },
      "source": [
        "print(data_proc_brown.input_text[1], data_proc_brown.input_idx[1])\n",
        "print(data_proc_brown.output_text[1], data_proc_brown.output_idx[1])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from the fact that the [ 55 146  48 144 146]\n",
            "the fact that the united [146  48 144 146 161]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAIxx6asoEKP"
      },
      "source": [
        "# Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5qmFxof_LhC"
      },
      "source": [
        "def get_batches(arr_x, arr_y, batch_size):\n",
        "         \n",
        "    # iterate through the arrays\n",
        "    prv = 0\n",
        "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
        "      x = arr_x[prv:n,:]\n",
        "      y = arr_y[prv:n,:]\n",
        "      prv = n\n",
        "      yield x, y"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLji7N2P_MK_"
      },
      "source": [
        "class WordLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, n_hidden=256, n_layers=4, drop_prob=0.3, \n",
        "                 lr=0.001):\n",
        "        super().__init__()\n",
        "\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
        "        self.lstm = nn.LSTM(200, n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "\n",
        "        ## pass input through embedding layer\n",
        "        embedded = self.emb_layer(x)     \n",
        "        \n",
        "        ## Get the outputs and the new hidden state from the lstm\n",
        "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
        "\n",
        "        out = self.dropout(lstm_output)\n",
        "        \n",
        "        #out = out.contiguous().view(-1, self.n_hidden) \n",
        "        out = out.reshape(-1, self.n_hidden) \n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        # if GPU is available\n",
        "        if (torch.cuda.is_available()):\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        \n",
        "        # if GPU is not available\n",
        "        else:\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "_9nFRrxE_QYV",
        "outputId": "f7b980db-d9dc-4039-b0e1-9314e5df0668"
      },
      "source": [
        "\n",
        "# instantiate the model\n",
        "plot_net = WordLSTM(data_proc.vocab_size).cuda()\n",
        "gutten_net = WordLSTM(data_proc.vocab_size).cuda()\n",
        "brown_net = WordLSTM(data_proc.vocab_size).cuda()\n",
        "print(plot_net)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-10fa1cf319b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# instantiate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgutten_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbrown_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'data_preprocessor' object has no attribute 'vocab_size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHFJNQ4o_SqL"
      },
      "source": [
        "def train(net, input_t, output_t, epochs=10, batch_size=32, lr=0.001, clip=1):\n",
        "    \n",
        "    optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    net.cuda()\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    epoch_loop = tqdm(range(epochs), desc=\"Epochs \")\n",
        "    batch_loop = tqdm(range(len(input_t)//batch_size), desc=\"Batch Status \")\n",
        "\n",
        "    for e in range(epochs):\n",
        "        batch_loop.refresh()\n",
        "        batch_loop.reset()\n",
        "        epoch_loop.update()\n",
        "\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(input_t, output_t, batch_size):\n",
        "            batch_loop.update()\n",
        "\n",
        "            # convert numpy arrays to PyTorch arrays\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # detach hidden states\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            net.zero_grad()\n",
        "            output, h = net(inputs, h)\n",
        "            loss = criterion(output, targets.view(-1))\n",
        "            loss.backward()\n",
        "\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "\n",
        "            optim.step()            \n",
        "            "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR7TPXWgpMeT"
      },
      "source": [
        "# Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoC2fHl9_W9u"
      },
      "source": [
        "# train the model\n",
        "train(plot_net,\n",
        "      data_proc.input_idx, \n",
        "      data_proc.output_idx, \n",
        "      batch_size = 32, epochs=30)\n",
        "\n",
        "torch.save(plot_net, \"/content/drive/MyDrive/NLP/plot_net.pth\")"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ADlFN6cqHhr"
      },
      "source": [
        "# train the model\n",
        "train(gutten_net,\n",
        "      data_proc_gut.input_idx, \n",
        "      data_proc_gut.output_idx, \n",
        "      batch_size = 32, epochs=20)\n",
        "\n",
        "\n",
        "torch.save(gutten_net, \"/content/drive/MyDrive/NLP/gutten_net.pth\")"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgZwXjFcqITI"
      },
      "source": [
        "# train the model\n",
        "train(brown_net,\n",
        "      data_proc_brown.input_idx, \n",
        "      data_proc_brown.output_idx, \n",
        "      batch_size = 32, epochs=20)\n",
        "\n",
        "torch.save(brown_net, \"/content/drive/MyDrive/NLP/brown_net.pth\")"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW9aYvlMbHO8"
      },
      "source": [
        "# Load Models and Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D9abLbtwwxw"
      },
      "source": [
        "### Reading model objects "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_fpgg-YbGdM"
      },
      "source": [
        "# plot_net = WordLSTM(data_proc.vocab_size).cuda()\n",
        "# plot_net = torch.load(\"plot_net.pth\")\n",
        "\n",
        "# gutten_net = WordLSTM(data_proc.vocab_size).cuda()\n",
        "# gutten_net = torch.load(\"gutten_net.pth\")\n",
        "\n",
        "# brown_net = WordLSTM(data_proc.vocab_size).cuda()\n",
        "# brown_net = torch.load(\"brown_net.pth\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8nItfqzw3mr"
      },
      "source": [
        "### Loading the test data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dDgLc8dklH2"
      },
      "source": [
        "test_gutten = pickle.load(open(\"test_orig_gutten.pickle\",\"rb\"))\n",
        "test_plot = pickle.load(open('test_orig_plot.pickle','rb'))\n",
        "test_brown = pickle.load(open(\"test_orig_brown.pickle\",\"rb\"))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Tm2EOeAEyoo"
      },
      "source": [
        "plot_net = WordLSTM(data_proc.vocab_size).cuda()\n",
        "plot_net = torch.load(\"plot_net_2.pth\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMob74UI3a11"
      },
      "source": [
        "### Preprocessing for test data\n",
        "Since the current RNN model is not trained to handel OOV items, data needs to pre-processed to avoid prediction error. This will result in reducing the test data size marginally. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwbeOY9wwEMf",
        "outputId": "da89ec93-89a5-4b01-a43a-f39ef5085e31"
      },
      "source": [
        "def process_test_data(sentence_list):\n",
        "  new_list = []\n",
        "  for sentence in sentence_list:\n",
        "    sentence = re.sub(\"[^a-z' ]\", \"\", sentence)\n",
        "    new_sent = []\n",
        "    for item in sentence.split():\n",
        "      if item not in [\",\",\".\",\"!\",\"?\"] and not item.isdigit():\n",
        "        new_sent.append(item.strip(\".,\"))\n",
        "    if len(new_sent) > 3:\n",
        "      new_list.append(\" \".join(new_sent))\n",
        "  return new_list\n",
        "\n",
        "test_brown_s = process_test_data(test_brown)\n",
        "test_gutten_s = process_test_data(test_gutten)\n",
        "test_plot_s = process_test_data(test_plot)\n",
        "\n",
        "print(f\"Original Length of corpus  :{len(test_brown)},{len(test_gutten)},{len(test_plot)}\")\n",
        "print(f\"Length after pre-processing:{len(test_brown_s)} ,{len(test_gutten_s)},{len(test_plot_s)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Length of corpus  :1000,1000,1000\n",
            "Length after pre-processing:983 ,1000,973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYaI1g8lxB1q"
      },
      "source": [
        "### Extracting firs two tokens from test sentences to use for generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygl-AscRlEE6"
      },
      "source": [
        "start_tokens_plot = [\" \".join(item.split()[:2]) for item in test_plot_s]\n",
        "start_tokens_gutten = [\" \".join(item.split()[:2]) for item in test_gutten_s]\n",
        "start_tokens_brown = [\" \".join(item.split()[:2]) for item in test_brown_s] "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPbqxhDJEh7J"
      },
      "source": [
        "start_tokens_plot_4 = [\" \".join(item.split()[:4]) for item in test_plot_s]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ALbRbzD_ZmL"
      },
      "source": [
        "# predict next token\n",
        "def predict(net, tkn, idx2wrd, wrd2idx ,h=None, sp=False):\n",
        "         \n",
        "  # tensor inputs\n",
        "  # x = np.array([[token2int[tkn]]])\n",
        "  x = np.array([[wrd2idx[tkn]]])\n",
        "  inputs = torch.from_numpy(x)\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "  print(f\"Input tkn: {tkn}   | Input index : {inputs}\") if sp else None\n",
        "  # detach hidden state from history\n",
        "  h = tuple([each.data for each in h])\n",
        "\n",
        "  out, h = net(inputs, h)\n",
        "\n",
        "  # get the token probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "\n",
        "  p = p.cpu()\n",
        "  p = p.numpy()\n",
        "  p = p.reshape(p.shape[1],)\n",
        "\n",
        "  # get indices of top 3 values\n",
        "  top_n_idx = p.argsort()[-3:][::-1]\n",
        "\n",
        "  # randomly select one of the three indices\n",
        "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
        "\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  # return int2token[sampled_token_index], h\n",
        "  return idx2wrd[sampled_token_index], h"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOfe6sISq7Pb"
      },
      "source": [
        "# function to generate text\n",
        "def sample(net, size, idx2word, word2idx, prime='it is', show_process = False):\n",
        "        \n",
        "    # push to GPU\n",
        "    net.cuda()\n",
        "    \n",
        "    net.eval()\n",
        "\n",
        "    # batch size is 1\n",
        "    h = net.init_hidden(1)\n",
        "\n",
        "    toks = prime.split()\n",
        "    print(f\"Initial State : {toks}\") if show_process else None\n",
        "    # predict next token\n",
        "    for t in prime.split():\n",
        "      token, h = predict(net, t, idx2word, word2idx, h, sp=show_process)\n",
        "    toks.append(token)\n",
        "    print(f\"Second state : {toks}\") if show_process else None\n",
        "\n",
        "    # predict subsequent tokens\n",
        "    for i in range(size-1):\n",
        "        inp = toks[-1]\n",
        "        token, h = predict(net, toks[-1],idx2word,word2idx, h, sp=show_process)\n",
        "        toks.append(token)\n",
        "        print(f\"Subsequent State | Input : {inp} \\\n",
        "        Output : {token}\") if show_process else None\n",
        "\n",
        "    return ' '.join(toks)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CRBTDDYB1kfV",
        "outputId": "ac93ccbb-d9a8-4f96-b554-3df51980df48"
      },
      "source": [
        "sample(plot_net, 15, data_proc.idx2word, data_proc.word2idx, \n",
        "       prime=\"she also\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'she also decides not to be able for a better and his family and completes his own'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "ebff93b22ede4bfa91250845749e6b5f",
            "68e645ef4c9f47729d0d2792ae386312",
            "4fc9928881be49d5ba4e0f0a1af00ab0",
            "26760fab1f95431a903e716890b12940",
            "60a38b0ee2d045b7a7553d1d5abc5045",
            "8c97b44c2cb842a681ab00033f971232",
            "4b8e6477899843bb96ad273a3b096c11",
            "c141a282594947f7864bea8dae198e8e",
            "439e8afb35764e1ab86d0989c8c27e7f",
            "34297d2d8a944d75a604fbcd5b3874a7",
            "18152586d89d41bf9fca2ca29c84532c",
            "a78e0e8c10d0486e92d891b5f70c6a63",
            "13e52acf015542ab8456b65166c479f8",
            "9b70bbe7f5f0481081bbba1c96a469cb",
            "09457583d66644a49d657b20cfa0a437",
            "b2650163694b4be7acb2048e51921db6",
            "46e24bfe7aed4a9482910f0c63cf7120",
            "f7665f6a5f1c40728e40fd7a330f6e4f",
            "1250e19a8c244707a09d5a0c9c2dc538",
            "a0769b66d0e44ada9c028d484a82405f",
            "3ddabc34ce874b3e9b80f517c5ac42a2",
            "5c84f71063ca416da01d8e6b8fef3fa1",
            "17b9f9615dca49ccbb0e9482987f5891",
            "b9f3734d4b42496d8e1eb32e86080869"
          ]
        },
        "id": "IiQUihl4lXT2",
        "outputId": "b1f2c67e-582f-4bd5-c56a-90ad2acd2b26"
      },
      "source": [
        "rnn_test_plot = []\n",
        "rnn_test_gutten = []\n",
        "rnn_test_brown = []\n",
        "\n",
        "for i in tqdm(range(len(start_tokens_plot))):\n",
        "  pred_plot = sample(plot_net, 15, data_proc.idx2word, \n",
        "                     data_proc.word2idx, prime=start_tokens_plot[i])\n",
        "  rnn_test_plot.append(pred_plot)\n",
        "\n",
        "for i in tqdm(range(len(start_tokens_gutten))):\n",
        "  pred_gutten = sample(gutten_net, 4, data_proc_gut.idx2word, \n",
        "                       data_proc_gut.word2idx, prime=start_tokens_gutten[i])\n",
        "\n",
        "  rnn_test_gutten.append(pred_gutten)\n",
        "\n",
        "for i in tqdm(range(len(start_tokens_brown))):\n",
        "  pred_brown = sample(brown_net, 4, data_proc_brown.idx2word, \n",
        "                      data_proc_brown.word2idx, prime=start_tokens_brown[i])\n",
        "  rnn_test_brown.append(pred_brown)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebff93b22ede4bfa91250845749e6b5f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=973.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "439e8afb35764e1ab86d0989c8c27e7f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46e24bfe7aed4a9482910f0c63cf7120",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=983.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "1c7e2d5cb7b6431ea160fc9b59a762e7",
            "cb4109b722c24db686e233cb011127ce",
            "8d1399f3f4e349e98387d0d22f2b61b4",
            "15ee459b05a54f088173e81337322c17",
            "a54fc994f45f40e89fbe9b233dac711e",
            "5768b8aa8e304a3189c82511696a8ecc",
            "bcf086333e784173b5bb0666601128e3",
            "33fa8035453d4c7fb338ed82020b360e"
          ]
        },
        "id": "5jgU3icqEoPn",
        "outputId": "cba4177d-189c-45c7-9b3a-3d0bb4f9bab7"
      },
      "source": [
        "rnn_test_plot_4 = []\n",
        "for i in tqdm(range(len(start_tokens_plot_4))):\n",
        "  pred_plot = sample(plot_net, 15, data_proc.idx2word, \n",
        "                     data_proc.word2idx, prime=start_tokens_plot_4[i])\n",
        "  rnn_test_plot_4.append(pred_plot)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c7e2d5cb7b6431ea160fc9b59a762e7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=973.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9YkilqIF_AB"
      },
      "source": [
        "## Printing the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OegYq1cBCisI"
      },
      "source": [
        "rnn_test_gutten = pickle.load(open(\"rnn_test_gutten.pickle\",\"rb\"))\n",
        "rnn_test_brown = pickle.load(open('rnn_test_brown.pickle','rb'))\n",
        "rnn_test_plot = pickle.load(open(\"rnn_test_plot.pickle\",\"rb\"))\n",
        "\n",
        "#reminder : Preprocess the test data first \n",
        "# test_brown_s = process_test_data(test_brown)\n",
        "# test_gutten_s = process_test_data(test_gutten)\n",
        "# test_plot_s = process_test_data(test_plot)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffe1D2CU-5yw"
      },
      "source": [
        "### Examples for RNN generated Movie plots with 2 inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXRN8Yk-wC5R",
        "outputId": "eebb4c73-128d-4f53-ca1b-406bf0e3392a"
      },
      "source": [
        "sample_ = np.random.choice(np.arange(len(test_plot)),5, replace=False)\n",
        "print(\"\\nResults for Movie plots with 2 token input : \\n\")\n",
        "for i in range(5):\n",
        "  print(f\"Original  :{test_plot_s[int(sample_[i])]}\")\n",
        "  print(f\"Generated : {rnn_test_plot[int(sample_[i])]}\\n\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Results for Movie plots : \n",
            "\n",
            "Original  :while adina tries to seduce a fellow kok brother jimmy to get the tape daisy is falling for the dog president leah\n",
            "Generated : while adina tries unsuccessfully to convince her how he is a software propounding a lot for a\n",
            "\n",
            "Original  :six months later a new game is discovered by a swat team led by lt\n",
            "Generated : six months and is paralyzed by a vampire and musician and the two are infected and quarantined\n",
            "\n",
            "Original  :the pace and tone of the film is immediately made clear with an opening fight in the woods as wahjee and his uncle attempt to flee from ruthless fighters led by mien tsumun\n",
            "Generated : the pace and rips the leader for a young boy in a car called the final murder\n",
            "\n",
            "Original  :this part starts with a zoom into a house and to a doghouse labeled killer with spike in it\n",
            "Generated : this part is in fact the same time in his home he was a priest and he\n",
            "\n",
            "Original  :during the funeral while having sex inside a closet kimie reveals to netah that the shimazakis were set up by the hanadas to be assassinated so they could take over the clan\n",
            "Generated : during the party the next night at first they are the most of his mother in a\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N0sWqhIGI8y"
      },
      "source": [
        "### Examples for RNN generated Movie plots with 4 inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcYix62jFwZJ",
        "outputId": "26603ce8-5446-48ab-fbe3-b7636dee836b"
      },
      "source": [
        "sample_ = np.random.choice(np.arange(len(test_plot)),5, replace=False)\n",
        "print(\"\\nResults for Movie plots with 4 token input: \\n\")\n",
        "for i in range(5):\n",
        "  print(f\"Original  :{test_plot_s[int(sample_[i])]}\")\n",
        "  print(f\"Generated : {rnn_test_plot_4[int(sample_[i])]}\\n\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Results for Movie plots : \n",
            "\n",
            "Original  :after one final moment of doubt when beth suggests they visit dr\n",
            "Generated : after one final moment of disassociative house and the rest are unsuccessful in the film is a successful depression\n",
            "\n",
            "Original  :jamal warns prettyeyed willy about chicago\n",
            "Generated : jamal warns prettyeyed willy and his wife are transporting dr connell eliminating the whole family is based for the\n",
            "\n",
            "Original  :although hurt she agrees and after a staged honeymoon aboard the dakin family yacht they return to boston\n",
            "Generated : although hurt she agrees in the village the film is the sensitive and completes a x interview the film\n",
            "\n",
            "Original  :jim admits to have been walking in the vicinity of the murder site that plus the cheque and his head injury make him the prime suspect\n",
            "Generated : jim admits to have a lot for the murder the two meet in the trunk of bobbili veerakesavudu who\n",
            "\n",
            "Original  :on the island local mario ruoppolo is dissatisfied with being a fisherman like his father\n",
            "Generated : on the island local title and his family and the two meet at a local comics party in the\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFArrGZwGLsE"
      },
      "source": [
        "### Examples for RNN generated Guttenberg with 2 inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAJGH7RA_dJ0",
        "outputId": "d3195bf6-2c00-4425-8dd2-fcf5de98a486"
      },
      "source": [
        "sample_ = np.random.choice(np.arange(len(test_plot)),5, replace=False)\n",
        "print(\"\\nResults for Guttenberg:\\n\")\n",
        "for i in range(5):\n",
        "  print(f\"Original  : {test_gutten_s[int(sample_[i])]}\")\n",
        "  print(f\"Generated : {rnn_test_gutten[int(sample_[i])]}\\n\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Results for Guttenberg:\n",
            "\n",
            "Original  : now i am sure i\n",
            "Generated : now i the able to be\n",
            "\n",
            "Original  : colonel brandon s being a bad\n",
            "Generated : colonel brandon s steele and the\n",
            "\n",
            "Original  : smith and if he had been\n",
            "Generated : smith and if he i not\n",
            "\n",
            "Original  : every thing that was not in\n",
            "Generated : every thing had not am to\n",
            "\n",
            "Original  : from the first time after all\n",
            "Generated : from the to be happy should\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-r-Yn0xGQhD"
      },
      "source": [
        "### Examples for RNN generated Brown with 2 inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEli2HVK_hxm",
        "outputId": "a0d57529-b705-4fbb-ed8e-c4ddbed5cc25"
      },
      "source": [
        "sample_ = np.random.choice(np.arange(len(test_brown)),5, replace=False)\n",
        "print(\"\\nResults for Brown:\\n\")\n",
        "for i in range(5):\n",
        "  print(f\"Original  : {test_brown_s[int(sample_[i])]}\")\n",
        "  print(f\"Generated : {rnn_test_brown[int(sample_[i])]}\\n\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Results for Brown:\n",
            "\n",
            "Original  : here the first time in\n",
            "Generated : here the first first the world\n",
            "\n",
            "Original  : at the same time the\n",
            "Generated : at the first first accepted and\n",
            "\n",
            "Original  : what is the only way to\n",
            "Generated : what is the only important of\n",
            "\n",
            "Original  : as a result of the united\n",
            "Generated : as a first time the first\n",
            "\n",
            "Original  : maybe i can see that the\n",
            "Generated : maybe i can see been the\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J3k5-s0GUDq"
      },
      "source": [
        "### Saving the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iJNBr349C00"
      },
      "source": [
        "with open('/content/drive/MyDrive/NLP/rnn_test_gutten.pickle', 'wb') as f:\n",
        "    pickle.dump(rnn_test_gutten, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP/rnn_test_brown.pickle', 'wb') as f:\n",
        "    pickle.dump(rnn_test_brown, f)\n",
        "    \n",
        "with open('/content/drive/MyDrive/NLP/rnn_test_plot.pickle', 'wb') as f:\n",
        "    pickle.dump(rnn_test_plot, f)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOCokRQ4qjK-"
      },
      "source": [
        "# Result Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-TUBE_r4Unu"
      },
      "source": [
        "## Evaluating RNN Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh6RwT-Lycse"
      },
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install bert_score"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_psvIF2EBJwC"
      },
      "source": [
        "# datasets require pyarrow version 2.0\n",
        "# import pyarrow\n",
        "# print(pyarrow.__version__) "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64ssWhCxrV35"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from datasets import load_metric"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOgdituQrBHJ"
      },
      "source": [
        "rnn_test_gutten = pickle.load(open(\"rnn_test_gutten.pickle\",\"rb\"))\n",
        "rnn_test_brown = pickle.load(open('rnn_test_brown.pickle','rb'))\n",
        "rnn_test_plot = pickle.load(open(\"rnn_test_plot.pickle\",\"rb\"))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gHwiSlFHz3T"
      },
      "source": [
        "### Calculating Bert score for **Guttenberg** results from RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YsxxC3m4AbB"
      },
      "source": [
        "bertscore_gutten = load_metric('bertscore')\n",
        "bertscore_gutten.add_batch(predictions=rnn_test_gutten, references=test_gutten_s)\n",
        "rnn_gutt_precision = torch.mean(bertscore_gutten.compute(lang='en')['precision']).item()\n",
        "bertscore_gutten = load_metric('bertscore')\n",
        "bertscore_gutten.add_batch(predictions=rnn_test_gutten, references=test_gutten_s)\n",
        "rnn_gutt_f1 = torch.mean(bertscore_gutten.compute(lang='en')['f1']).item()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhAqhhwYH60J"
      },
      "source": [
        "### Calculating Bert score for **Brown** results from RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se5LdmMXygDZ"
      },
      "source": [
        "bertscore_brown = load_metric('bertscore')\n",
        "bertscore_brown.add_batch(predictions=rnn_test_brown, references=test_brown_s)\n",
        "rnn_brown_precision = torch.mean(bertscore_brown.compute(lang='en')['precision']).item()\n",
        "bertscore_brown = load_metric('bertscore')\n",
        "bertscore_brown.add_batch(predictions=rnn_test_brown, references=test_brown_s)\n",
        "rnn_brown_f1 = torch.mean(bertscore_brown.compute(lang='en')['f1']).item()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpw50e1nH-bJ"
      },
      "source": [
        "### Calculating Bert score for **Movie plot (2 token input)** results from RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlMdIBIA0DDM"
      },
      "source": [
        "bertscore_plot = load_metric('bertscore')\n",
        "bertscore_plot.add_batch(predictions=rnn_test_plot, references=test_plot_s)\n",
        "rnn_plot_precision = torch.mean(bertscore_plot.compute(lang='en')['precision']).item()\n",
        "bertscore_plot = load_metric('bertscore')\n",
        "bertscore_plot.add_batch(predictions=rnn_test_plot, references=test_plot_s)\n",
        "rnn_plot_f1 = torch.mean(bertscore_plot.compute(lang='en')['f1']).item()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St5ZiK0PIDBS"
      },
      "source": [
        "### Calculating Bert score for **Movie plot (4 token input)** results from RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p5Ys-1UGc-l"
      },
      "source": [
        "bertscore_plot = load_metric('bertscore')\n",
        "bertscore_plot.add_batch(predictions=rnn_test_plot_4, references=test_plot_s)\n",
        "rnn_plot_4_precision = torch.mean(bertscore_plot.compute(lang='en')['precision']).item()\n",
        "bertscore_plot = load_metric('bertscore')\n",
        "bertscore_plot.add_batch(predictions=rnn_test_plot_4, references=test_plot_s)\n",
        "rnn_plot_4_f1 = torch.mean(bertscore_plot.compute(lang='en')['f1']).item()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1saqMVDxUiy",
        "outputId": "dee41cc0-7557-4352-bff8-025c16e1ec61"
      },
      "source": [
        "print(\"\\n\")\n",
        "print(f\"RNN | Precision | Movie Plot 2 token| {round(rnn_plot_precision,3)}\")\n",
        "print(f\"RNN | Precision | Movie Plot 4 token| {round(rnn_plot_4_precision,3)}\")\n",
        "print(f\"RNN | Precision | Guttenberg 2 token| {round(rnn_gutt_precision,3)}\")\n",
        "print(f\"RNN | Precision | Brown Crop 2 token| {round(rnn_brown_precision,3)}\")\n",
        "print(\"\\n\")\n",
        "print(f\"RNN | F-1 score | Movie Plot 2 token| {round(rnn_plot_f1,3)}\")\n",
        "print(f\"RNN | F-1 score | Movie Plot 4 token| {round(rnn_plot_4_f1,3)}\")\n",
        "print(f\"RNN | F-1 score | Guttenberg 2 token| {round(rnn_gutt_f1,3)}\")\n",
        "print(f\"RNN | F-1 score | Brown Crop 2 token| {round(rnn_brown_f1,3)}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "RNN | Precision | Movie Plot 2 token| 0.845\n",
            "RNN | Precision | Movie Plot 4 token| 0.856\n",
            "RNN | Precision | Guttenberg 2 token| 0.857\n",
            "RNN | Precision | Brown Crop 2 token| 0.852\n",
            "\n",
            "\n",
            "RNN | F-1 score | Movie Plot 2 token| 0.842\n",
            "RNN | F-1 score | Movie Plot 4 token| 0.856\n",
            "RNN | F-1 score | Guttenberg 2 token| 0.861\n",
            "RNN | F-1 score | Brown Crop 2 token| 0.856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IVUz5dL4Yja"
      },
      "source": [
        "## Evaluating HMM Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lYzrGhT0924"
      },
      "source": [
        "gen_test_gutten = pickle.load(open(\"test_gen_gutten.pickle\",\"rb\"))\n",
        "gen_test_gutten = [\" \".join(item) for item in gen_test_gutten]\n",
        "\n",
        "gen_test_brown = pickle.load(open(\"test_gen_brown.pickle\",\"rb\"))\n",
        "gen_test_brown = [\" \".join(item) for item in gen_test_brown]\n",
        "\n",
        "gen_test_plot = pickle.load(open(\"test_gen_plot_unrestricted.pickle\",\"rb\"))\n",
        "gen_test_plot = [\" \".join(item) for item in gen_test_plot]\n",
        "\n",
        "test_gutten = pickle.load(open(\"test_orig_gutten.pickle\",\"rb\"))\n",
        "test_brown = pickle.load(open(\"test_orig_brown.pickle\",\"rb\"))\n",
        "test_plot = pickle.load(open('test_orig_plot.pickle','rb'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhF9Z9lqISsQ"
      },
      "source": [
        "### Calculating Bert score for **Guttenberg** results from Generative model (HMM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXVrizsc45dr"
      },
      "source": [
        "bertscore_gutten_gen = load_metric('bertscore')\n",
        "bertscore_gutten_gen.add_batch(predictions=gen_test_gutten, references=test_gutten)\n",
        "gen_gutt_precision =  torch.mean(bertscore_gutten_gen.compute(lang='en')['precision']).item()\n",
        "bertscore_gutten_gen = load_metric('bertscore')\n",
        "bertscore_gutten_gen.add_batch(predictions=gen_test_gutten, references=test_gutten)\n",
        "gen_gutt_f1 =  torch.mean(bertscore_gutten_gen.compute(lang='en')['f1']).item()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogz4ORSoIckw"
      },
      "source": [
        "### Calculating Bert score for **Brown** results from Generative model (HMM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxZx1HmY6bHr"
      },
      "source": [
        "bertscore_brown_gen = load_metric('bertscore')\n",
        "bertscore_brown_gen.add_batch(predictions=gen_test_brown, references=test_brown)\n",
        "gen_brown_precision =  torch.mean(bertscore_brown_gen.compute(lang='en')['precision']).item()\n",
        "bertscore_brown_gen = load_metric('bertscore')\n",
        "bertscore_brown_gen.add_batch(predictions=gen_test_brown, references=test_brown)\n",
        "gen_brown_f1 =  torch.mean(bertscore_brown_gen.compute(lang='en')['f1']).item()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpO9R4rfIhLR"
      },
      "source": [
        "### Calculating Bert score for **Movie plot** results from Generative model (HMM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwafdjNU6wTd"
      },
      "source": [
        "bertscore_plot_gen = load_metric('bertscore')\n",
        "bertscore_plot_gen.add_batch(predictions=gen_test_plot, references=test_plot)\n",
        "gen_plot_precision =  torch.mean(bertscore_plot_gen.compute(lang='en')['precision']).item()\n",
        "bertscore_plot_gen = load_metric('bertscore')\n",
        "bertscore_plot_gen.add_batch(predictions=gen_test_plot, references=test_plot)\n",
        "gen_plot_f1 =  torch.mean(bertscore_plot_gen.compute(lang='en')['f1']).item()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gmui69BR8a1V",
        "outputId": "ae5e86ba-59c0-4291-c048-4d3483391e66"
      },
      "source": [
        "print(\"\\n\")\n",
        "print(f\"Generative | Precision | Movie Plot | {round(gen_plot_precision,3)}\")\n",
        "print(f\"Generative | Precision | Guttenberg | {round(gen_gutt_precision,3)}\")\n",
        "print(f\"Generative | Precision | Brown Crop | {round(gen_brown_precision,3)}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Generative | F-1 score | Movie Plot | {round(gen_plot_f1,3)}\")\n",
        "print(f\"Generative | F-1 score | Guttenberg | {round(gen_gutt_f1,3)}\")\n",
        "print(f\"Generative | F-1 score | Brown Crop | {round(gen_brown_f1,3)}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Generative | Precision | Movie Plot | 0.846\n",
            "Generative | Precision | Guttenberg | 1.0\n",
            "Generative | Precision | Brown Crop | 0.851\n",
            "\n",
            "\n",
            "Generative | F-1 score | Movie Plot | 0.852\n",
            "Generative | F-1 score | Guttenberg | 1.0\n",
            "Generative | F-1 score | Brown Crop | 0.853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaRyFQ_PETLq",
        "outputId": "3758130e-2ccb-41f7-b9c7-9c979e6d8a8f"
      },
      "source": [
        "print(\"\\n\")\n",
        "print(f\"RNN | Precision | Movie Plot 2 token| {round(rnn_plot_precision,3)}\")\n",
        "print(f\"RNN | Precision | Movie Plot 4 token| {round(rnn_plot_4_precision,3)}\")\n",
        "print(f\"RNN | Precision | Guttenberg 2 token| {round(rnn_gutt_precision,3)}\")\n",
        "print(f\"RNN | Precision | Brown Crop 2 token| {round(rnn_brown_precision,3)}\")\n",
        "print(\"\\n\")\n",
        "print(f\"RNN | F-1 score | Movie Plot 2 token| {round(rnn_plot_f1,3)}\")\n",
        "print(f\"RNN | F-1 score | Movie Plot 4 token| {round(rnn_plot_4_f1,3)}\")\n",
        "print(f\"RNN | F-1 score | Guttenberg 2 token| {round(rnn_gutt_f1,3)}\")\n",
        "print(f\"RNN | F-1 score | Brown Crop 2 token| {round(rnn_brown_f1,3)}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "RNN | Precision | Movie Plot 2 token| 0.845\n",
            "RNN | Precision | Movie Plot 4 token| 0.856\n",
            "RNN | Precision | Guttenberg 2 token| 0.857\n",
            "RNN | Precision | Brown Crop 2 token| 0.852\n",
            "\n",
            "\n",
            "RNN | F-1 score | Movie Plot 2 token| 0.842\n",
            "RNN | F-1 score | Movie Plot 4 token| 0.856\n",
            "RNN | F-1 score | Guttenberg 2 token| 0.861\n",
            "RNN | F-1 score | Brown Crop 2 token| 0.856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZugFWjn5zH2p"
      },
      "source": [
        "*****"
      ]
    }
  ]
}